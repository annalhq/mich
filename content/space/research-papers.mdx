---
title: "Research papers"
description: "my reading list of research papers"
date: "2025-04-21"
---

## My reading list of research papers

- [SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement](https://arxiv.org/abs/2504.07934)
- [Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2504.12216)
---

### Older ones

- [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- [Masked Siamese networks for label-efficient learning](https://arxiv.org/abs/2204.07141)
     > A new state-of-the-art for self-supervised learning on this benchmark while using nearly 10x fewer parameters than previous best performing approaches and 100x fewer labeled images than current mask-based auto-encode [2022]
- [Rationale behind LoRA](https://arxiv.org/abs/2012.13255)
- [GZip-kNN](https://blog.codingconfessions.com/p/decoding-the-acl-paper-gzip-and-knn)
- Perceptron and Sparse probabilty
     > Training a multilayer perceptron with a single hidden layer corresponds to the evolution of a sparse probability measure (sum of Dirac masses) over the neuron's parameter domain (here it is 2D for the regression of a 1D function, slope+position of the ridge).
- [MM1 by Apple](https://arxiv.org/abs/2403.09611)
- Grok 1 Architecture
     > - Uses MoE layer to enhance training efficientcy of LLMs
     > 1. Attention is scaled by 30/tanh(x/30) ?!
     > 2. Approx GELU is used like Gemma
     > 3. 4x Layernoms unlike 2x for Llama
     > 4. RMS Layernorm downcasts at the end unlike Llama - same as Gemma
     > 5. RoPE is fully in float32 I think like Gemma
     > 6. Multipliers are 1
     > 7. QKV has bias, O no bias MLP no bias
     > 8. Vocab size is 131072. Gemma 256000.
